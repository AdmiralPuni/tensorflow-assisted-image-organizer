{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char-5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1RVo84MJ9UFRLx37eBFHISW1hhcsHREFk",
      "authorship_tag": "ABX9TyNWUR+uZe+xAGHjgbEq/GNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdmiralPuni/tensorflow-assisted-image-organizer/blob/master/char_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVgICmzMDcNL"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TBS2b-_Dgwi"
      },
      "source": [
        "import zipfile,os\n",
        "local_zip = '/content/drive/MyDrive/Colab Notebooks/data/flip.zip' #change the zip filename\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqpkxdPJDfc4"
      },
      "source": [
        "train_dir = '/content/flip' #change the middle one to the zip parent folder"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCIp6BpyDmf0"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "                    validation_split=0.3,\n",
        "                    rescale=1./255)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC3o-OZqDn1k",
        "outputId": "121b2d63-a9d2-4402-ec66-c991f18a474c"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=32,\n",
        "                                                    subset='training',\n",
        "                                                    shuffle=True)\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                         class_mode='categorical',\n",
        "                                                         target_size=(150, 150),\n",
        "                                                         batch_size=32,\n",
        "                                                         subset='validation',\n",
        "                                                         shuffle=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6720 images belonging to 32 classes.\n",
            "Found 2880 images belonging to 32 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN5dMJqqDpEq"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='softmax') #change the number to the number of characters in your zip file\n",
        "])"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW1e6Lv3DqSc"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.optimizers.Adam(),\n",
        "              metrics=['categorical_accuracy'])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7sWWhBYDUOC"
      },
      "source": [
        "filepath = \"/content/model-hololive-flip.hdf5\" #change the model name after 'model-' to the zip filename\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "desired_callbacks = [checkpoint]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ljhffbjDrab",
        "outputId": "7815755e-19e9-4935-c42e-2144d712aee5"
      },
      "source": [
        "model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=64,\n",
        "      epochs=32,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=64,\n",
        "      verbose=1,\n",
        "      callbacks=desired_callbacks)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n",
            "64/64 [==============================] - 111s 2s/step - loss: 3.2956 - categorical_accuracy: 0.0752 - val_loss: 3.0590 - val_categorical_accuracy: 0.1304\n",
            "\n",
            "Epoch 00001: loss improved from inf to 3.29565, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 2/32\n",
            "64/64 [==============================] - 115s 2s/step - loss: 2.8636 - categorical_accuracy: 0.1733 - val_loss: 2.7054 - val_categorical_accuracy: 0.2251\n",
            "\n",
            "Epoch 00002: loss improved from 3.29565 to 2.86359, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 3/32\n",
            "64/64 [==============================] - 115s 2s/step - loss: 2.5236 - categorical_accuracy: 0.2549 - val_loss: 2.4877 - val_categorical_accuracy: 0.2500\n",
            "\n",
            "Epoch 00003: loss improved from 2.86359 to 2.52363, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 4/32\n",
            "64/64 [==============================] - 113s 2s/step - loss: 2.2455 - categorical_accuracy: 0.3262 - val_loss: 2.2111 - val_categorical_accuracy: 0.3433\n",
            "\n",
            "Epoch 00004: loss improved from 2.52363 to 2.24550, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 5/32\n",
            "64/64 [==============================] - 113s 2s/step - loss: 1.8968 - categorical_accuracy: 0.4307 - val_loss: 2.1844 - val_categorical_accuracy: 0.3813\n",
            "\n",
            "Epoch 00005: loss improved from 2.24550 to 1.89680, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 6/32\n",
            "64/64 [==============================] - 114s 2s/step - loss: 1.8379 - categorical_accuracy: 0.4604 - val_loss: 1.9395 - val_categorical_accuracy: 0.4375\n",
            "\n",
            "Epoch 00006: loss improved from 1.89680 to 1.83792, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 7/32\n",
            "64/64 [==============================] - 115s 2s/step - loss: 1.5553 - categorical_accuracy: 0.5361 - val_loss: 1.7265 - val_categorical_accuracy: 0.5049\n",
            "\n",
            "Epoch 00007: loss improved from 1.83792 to 1.55529, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 8/32\n",
            "64/64 [==============================] - 116s 2s/step - loss: 1.3295 - categorical_accuracy: 0.5830 - val_loss: 1.6196 - val_categorical_accuracy: 0.5371\n",
            "\n",
            "Epoch 00008: loss improved from 1.55529 to 1.32953, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 9/32\n",
            "64/64 [==============================] - 115s 2s/step - loss: 1.2169 - categorical_accuracy: 0.6279 - val_loss: 1.5752 - val_categorical_accuracy: 0.5498\n",
            "\n",
            "Epoch 00009: loss improved from 1.32953 to 1.21690, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 10/32\n",
            "64/64 [==============================] - 114s 2s/step - loss: 0.9861 - categorical_accuracy: 0.7002 - val_loss: 1.4861 - val_categorical_accuracy: 0.5884\n",
            "\n",
            "Epoch 00010: loss improved from 1.21690 to 0.98611, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 11/32\n",
            "64/64 [==============================] - 114s 2s/step - loss: 0.8666 - categorical_accuracy: 0.7500 - val_loss: 1.4407 - val_categorical_accuracy: 0.5972\n",
            "\n",
            "Epoch 00011: loss improved from 0.98611 to 0.86662, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 12/32\n",
            "64/64 [==============================] - 116s 2s/step - loss: 0.8088 - categorical_accuracy: 0.7554 - val_loss: 1.3436 - val_categorical_accuracy: 0.6387\n",
            "\n",
            "Epoch 00012: loss improved from 0.86662 to 0.80878, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 13/32\n",
            "64/64 [==============================] - 116s 2s/step - loss: 0.6815 - categorical_accuracy: 0.7974 - val_loss: 1.5831 - val_categorical_accuracy: 0.6050\n",
            "\n",
            "Epoch 00013: loss improved from 0.80878 to 0.68153, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 14/32\n",
            "64/64 [==============================] - 115s 2s/step - loss: 0.6958 - categorical_accuracy: 0.7793 - val_loss: 1.3445 - val_categorical_accuracy: 0.6406\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.68153\n",
            "Epoch 15/32\n",
            "64/64 [==============================] - 116s 2s/step - loss: 0.4912 - categorical_accuracy: 0.8574 - val_loss: 1.4916 - val_categorical_accuracy: 0.6484\n",
            "\n",
            "Epoch 00015: loss improved from 0.68153 to 0.49120, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 16/32\n",
            "64/64 [==============================] - 116s 2s/step - loss: 0.5019 - categorical_accuracy: 0.8564 - val_loss: 1.4639 - val_categorical_accuracy: 0.6509\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.49120\n",
            "Epoch 17/32\n",
            "64/64 [==============================] - 114s 2s/step - loss: 0.4514 - categorical_accuracy: 0.8638 - val_loss: 1.5509 - val_categorical_accuracy: 0.6299\n",
            "\n",
            "Epoch 00017: loss improved from 0.49120 to 0.45137, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 18/32\n",
            "64/64 [==============================] - 114s 2s/step - loss: 0.4345 - categorical_accuracy: 0.8691 - val_loss: 1.3257 - val_categorical_accuracy: 0.6924\n",
            "\n",
            "Epoch 00018: loss improved from 0.45137 to 0.43452, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 19/32\n",
            "64/64 [==============================] - 115s 2s/step - loss: 0.3723 - categorical_accuracy: 0.8916 - val_loss: 1.2984 - val_categorical_accuracy: 0.6846\n",
            "\n",
            "Epoch 00019: loss improved from 0.43452 to 0.37229, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 20/32\n",
            "64/64 [==============================] - 115s 2s/step - loss: 0.2947 - categorical_accuracy: 0.9199 - val_loss: 1.4591 - val_categorical_accuracy: 0.7065\n",
            "\n",
            "Epoch 00020: loss improved from 0.37229 to 0.29473, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 21/32\n",
            "64/64 [==============================] - 113s 2s/step - loss: 0.3218 - categorical_accuracy: 0.9165 - val_loss: 1.2757 - val_categorical_accuracy: 0.7197\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.29473\n",
            "Epoch 22/32\n",
            "64/64 [==============================] - 111s 2s/step - loss: 0.2901 - categorical_accuracy: 0.9165 - val_loss: 1.5364 - val_categorical_accuracy: 0.6538\n",
            "\n",
            "Epoch 00022: loss improved from 0.29473 to 0.29012, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 23/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.2667 - categorical_accuracy: 0.9214 - val_loss: 1.2196 - val_categorical_accuracy: 0.7349\n",
            "\n",
            "Epoch 00023: loss improved from 0.29012 to 0.26674, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 24/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.2706 - categorical_accuracy: 0.9180 - val_loss: 1.2140 - val_categorical_accuracy: 0.7451\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.26674\n",
            "Epoch 25/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.2312 - categorical_accuracy: 0.9360 - val_loss: 1.4663 - val_categorical_accuracy: 0.6807\n",
            "\n",
            "Epoch 00025: loss improved from 0.26674 to 0.23118, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 26/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.2293 - categorical_accuracy: 0.9375 - val_loss: 1.3725 - val_categorical_accuracy: 0.7319\n",
            "\n",
            "Epoch 00026: loss improved from 0.23118 to 0.22933, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 27/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.2039 - categorical_accuracy: 0.9473 - val_loss: 1.2950 - val_categorical_accuracy: 0.7319\n",
            "\n",
            "Epoch 00027: loss improved from 0.22933 to 0.20393, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 28/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.1899 - categorical_accuracy: 0.9497 - val_loss: 1.3841 - val_categorical_accuracy: 0.7305\n",
            "\n",
            "Epoch 00028: loss improved from 0.20393 to 0.18994, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 29/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.1868 - categorical_accuracy: 0.9512 - val_loss: 1.2822 - val_categorical_accuracy: 0.7461\n",
            "\n",
            "Epoch 00029: loss improved from 0.18994 to 0.18680, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 30/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.1319 - categorical_accuracy: 0.9653 - val_loss: 1.3730 - val_categorical_accuracy: 0.7236\n",
            "\n",
            "Epoch 00030: loss improved from 0.18680 to 0.13194, saving model to /content/model-hololive-flip.hdf5\n",
            "Epoch 31/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.2033 - categorical_accuracy: 0.9424 - val_loss: 1.4182 - val_categorical_accuracy: 0.7217\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.13194\n",
            "Epoch 32/32\n",
            "64/64 [==============================] - 110s 2s/step - loss: 0.1966 - categorical_accuracy: 0.9482 - val_loss: 1.3155 - val_categorical_accuracy: 0.7222\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.13194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f085b841d10>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}